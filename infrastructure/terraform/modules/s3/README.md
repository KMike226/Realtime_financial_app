# Module S3 Data Lake avec Partitioning Intelligent

Ce module Terraform configure un Data Lake S3 avec une structure de partitioning intelligente optimis√©e pour les donn√©es financi√®res en temps r√©el.

## Fonctionnalit√©s

### üóÇÔ∏è Structure de Partitioning Intelligente

Le module impl√©mente une strat√©gie de partitioning hi√©rarchique qui organise automatiquement les donn√©es selon plusieurs dimensions :

#### Structure Raw Data
```
raw/
‚îú‚îÄ‚îÄ financial/
‚îÇ   ‚îî‚îÄ‚îÄ year=2025/month=03/day=15/hour=14/symbol=AAPL/
‚îÇ       ‚îú‚îÄ‚îÄ AAPL_20250315_140530_123.json
‚îÇ       ‚îî‚îÄ‚îÄ AAPL_20250315_140532_456.json
‚îî‚îÄ‚îÄ crypto/
    ‚îî‚îÄ‚îÄ year=2025/month=03/day=15/hour=14/symbol=BTC/
        ‚îú‚îÄ‚îÄ BTC_20250315_140530_789.json
        ‚îî‚îÄ‚îÄ BTC_20250315_140532_012.json
```

#### Structure Processed Data
```
processed/
‚îú‚îÄ‚îÄ data_type=indicators/
‚îÇ   ‚îî‚îÄ‚îÄ year=2025/month=03/day=15/
‚îÇ       ‚îú‚îÄ‚îÄ RSI_AAPL_20250315.json
‚îÇ       ‚îî‚îÄ‚îÄ MACD_BTC_20250315.json
‚îú‚îÄ‚îÄ data_type=alerts/
‚îÇ   ‚îî‚îÄ‚îÄ year=2025/month=03/day=15/
‚îÇ       ‚îî‚îÄ‚îÄ price_alerts_20250315.json
‚îî‚îÄ‚îÄ analytics/
    ‚îî‚îÄ‚îÄ report_type=daily_summary/year=2025/month=03/
        ‚îî‚îÄ‚îÄ market_summary_20250315.json
```

### üîÑ Lifecycle Management Automatis√©

Le module configure des politiques de cycle de vie diff√©renci√©es selon le type de donn√©es :

#### Donn√©es Financi√®res Traditionnelles
- **Standard** (0-30 jours) : Acc√®s fr√©quent
- **Standard IA** (30-90 jours) : Acc√®s occasionnel  
- **Glacier** (90-365 jours) : Archivage
- **Deep Archive** (365+ jours) : Archivage long terme
- **Suppression** apr√®s 7 ans

#### Donn√©es Crypto
- **Standard** (0-7 jours) : Acc√®s tr√®s fr√©quent
- **Standard IA** (7-30 jours) : Acc√®s fr√©quent
- **Glacier** (30-180 jours) : Archivage
- **Deep Archive** (180+ jours) : Archivage long terme
- **Suppression** apr√®s 3 ans

#### Donn√©es Process√©es
- **Standard** (0-90 jours) : Acc√®s pour analytics
- **Standard IA** (90-365 jours) : Acc√®s occasionnel
- **Glacier** (365+ jours) : Archivage long terme
- **Suppression** apr√®s 10 ans

### üîî Notifications Automatiques

Le module configure des notifications S3 pour d√©clencher automatiquement le traitement des nouvelles donn√©es :

- **Nouvelles donn√©es financi√®res** ‚Üí Trigger Lambda de partitioning
- **Nouvelles donn√©es crypto** ‚Üí Trigger Lambda de partitioning
- **Support des formats JSON** avec filtrage automatique

### üîí S√©curit√© et Contr√¥le d'Acc√®s

#### Chiffrement
- **AES-256** pour tous les objets
- **Chiffrement c√¥t√© serveur** automatique
- **Bucket key enabled** pour optimiser les co√ªts

#### Politiques d'Acc√®s
- **Blocage des acc√®s publics** complet
- **Acc√®s r√¥le-bas√©** pour les services de traitement
- **Permissions granulaires** par type de donn√©es

## Variables d'Entr√©e

| Variable | Type | Description | D√©faut |
|----------|------|-------------|--------|
| `project_name` | string | Nom du projet | - |
| `environment` | string | Environnement (dev, staging, prod) | - |
| `partition_lambda_arn` | string | ARN de la Lambda de partitioning | "" |
| `processing_role_arn` | string | ARN du r√¥le de traitement | "*" |
| `analytics_role_arn` | string | ARN du r√¥le d'analytics | "*" |

## Outputs

| Output | Description |
|--------|-------------|
| `bucket_name` | Nom du bucket Data Lake |
| `bucket_arn` | ARN du bucket Data Lake |
| `processed_bucket_name` | Nom du bucket donn√©es process√©es |
| `processed_bucket_arn` | ARN du bucket donn√©es process√©es |
| `bucket_domain_name` | Nom de domaine du bucket |
| `partition_structure` | Structure de partitioning recommand√©e |
| `lifecycle_policies` | R√©sum√© des politiques de cycle de vie |

## Utilisation

### Configuration de Base

```hcl
module "data_lake" {
  source = "./modules/s3"

  project_name = "financial-pipeline"
  environment  = "dev"
}
```

### Configuration Avanc√©e avec Lambda

```hcl
module "data_lake" {
  source = "./modules/s3"

  project_name         = "financial-pipeline"
  environment          = "dev"
  partition_lambda_arn = aws_lambda_function.partition_lambda.arn
  processing_role_arn  = aws_iam_role.processing_role.arn
  analytics_role_arn   = aws_iam_role.analytics_role.arn
}
```

## Script de Partitioning

Le module inclut un script Python (`scripts/partition-data.py`) pour g√©rer le partitioning intelligent :

### Utilisation en Ligne de Commande

```bash
# Partitioning d'un fichier sp√©cifique
python scripts/partition-data.py --bucket my-data-lake --file raw/upload/data.json

# Partitioning en lot
python scripts/partition-data.py --bucket my-data-lake --prefix raw/upload/ --max-files 50
```

### Utilisation comme Lambda

Le script peut √™tre d√©ploy√© comme fonction AWS Lambda pour traiter automatiquement les nouvelles donn√©es via les notifications S3.

### Fonctionnalit√©s du Script

- **D√©tection automatique** du type de donn√©es
- **Extraction intelligente** des m√©tadonn√©es (timestamp, symbole, etc.)
- **Gestion des erreurs** robuste
- **Support multi-format** de timestamps
- **√âvitement des doublons** et optimisations

## Optimisations de Performance

### Requ√™tes Efficaces

La structure de partitioning permet des requ√™tes optimis√©es :

```sql
-- Requ√™te optimis√©e avec partition pruning
SELECT * FROM financial_data 
WHERE year = 2025 
  AND month = 3 
  AND day = 15 
  AND symbol = 'AAPL'
```

### Strat√©gies de Lecture

- **Lecture parall√®le** par partition
- **Filtrage pr√©coce** sur les m√©tadonn√©es
- **Minimisation du scan** de donn√©es

## Monitoring et Alertes

### M√©triques CloudWatch

Le module supporte le monitoring via :
- **Nombre d'objets** par partition
- **Taille des donn√©es** par type
- **Latence d'acc√®s** aux donn√©es
- **Co√ªts de stockage** par lifecycle tier

### Alertes Recommand√©es

- **Croissance anormale** de certaines partitions
- **√âchecs de partitioning** r√©p√©t√©s  
- **D√©passement des co√ªts** de stockage
- **Latence d'acc√®s** √©lev√©e

## Maintenance

### Nettoyage P√©riodique

```bash
# V√©rification de l'int√©grit√© des partitions
aws s3 ls s3://bucket-name/raw/ --recursive | grep -v "year="

# Statistiques par partition
aws s3 ls s3://bucket-name/raw/financial/ --recursive --summarize
```

### Optimisation des Co√ªts

- **R√©vision p√©riodique** des policies de lifecycle
- **Analyse des patterns** d'acc√®s aux donn√©es
- **Ajustement des transitions** selon l'usage r√©el

## Bonnes Pratiques

### Nomenclature des Fichiers

- **Format recommand√©** : `SYMBOL_YYYYMMDD_HHMMSS_microseconds.json`
- **Caract√®res autoris√©s** : alphanum√©riques, tirets, underscores
- **Casse recommand√©e** : symboles en majuscules

### Structure des Donn√©es JSON

```json
{
  "timestamp": "2025-03-15T14:30:45.123Z",
  "symbol": "AAPL",
  "price": 150.25,
  "volume": 1000000,
  "source": "alpha_vantage",
  "data_type": "quote"
}
```

### Gestion des Erreurs

- **Retry automatique** pour les erreurs temporaires
- **Dead letter queue** pour les √©checs persistants
- **Logging d√©taill√©** pour le debugging
- **Alertes** sur les patterns d'erreurs

## Troubleshooting

### Probl√®mes Courants

#### Permissions Insuffisantes
```bash
# V√©rifier les permissions IAM
aws iam simulate-principal-policy \
  --policy-source-arn arn:aws:iam::account:role/role-name \
  --action-names s3:PutObject \
  --resource-arns arn:aws:s3:::bucket-name/raw/financial/*
```

#### Notifications Non D√©clench√©es
```bash
# V√©rifier la configuration des notifications
aws s3api get-bucket-notification-configuration --bucket bucket-name
```

#### Performance de Requ√™te
```bash
# Analyser la distribution des partitions
aws s3 ls s3://bucket-name/raw/financial/ --recursive | \
  awk -F'/' '{print $3"/"$4"/"$5}' | sort | uniq -c
```

## Migration et √âvolution

### Migration depuis Structure Existante

Le script de partitioning supporte la migration de donn√©es existantes :

```bash
# Migration progressive par batch
python scripts/partition-data.py \
  --bucket existing-bucket \
  --prefix legacy/ \
  --max-files 100
```

### √âvolution du Schema

- **Backward compatibility** maintenue
- **Versioning** des structures de partitioning
- **Migration progressive** des donn√©es legacy
- **Tests de regression** automatis√©s
